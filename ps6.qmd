---
title: "Problem Set 6 - Waze Shiny Dashboard"
author: "Peter Ganong, Maggie Shi, and Andre Oviedo"
date: November 23, 2024
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---
1. **ps6:** Due Sat 23rd at 5:00PM Central. Worth 100 points (80 points from questions, 10 points for correct submission and 10 points for code style) + 10 extra credit. 

We use (`*`) to indicate a problem that we think might be time consuming. 

# Steps to submit (10 points on PS6) {-}

1. "This submission is my work alone and complies with the 30538 integrity
policy." Add your initials to indicate your agreement: \*\*sv\*\*
2. "I have uploaded the names of anyone I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  \*\*sv\*\* (2 point)
3. Late coins used this pset: \*\*1/4\*\* Late coins left after submission: \*\*3/4\*\*

4. Before starting the problem set, make sure to read and agree to the terms of data usage for the Waze data [here](https://canvas.uchicago.edu/courses/59054/quizzes/130617).

5. Knit your `ps6.qmd` as a pdf document and name it `ps6.pdf`.
6. Submit your `ps6.qmd`, `ps6.pdf`, `requirements.txt`, and all created folders (we will create three Shiny apps so you will have at least three additional folders) to the gradescope repo assignment (5 points).
7. Submit `ps6.pdf` and also link your Github repo via Gradescope (5 points)
8. Tag your submission in Gradescope. For the Code Style part (10 points) please tag the whole correspondingsection for the code style rubric.

*Notes: see the [Quarto documentation (link)](https://quarto.org/docs/authoring/figures.html) for directions on inserting images into your knitted document.*

*IMPORTANT: For the App portion of the PS, in case you can not arrive to the expected functional dashboard we will need to take a look at your `app.py` file. You can use the following code chunk template to "import" and print the content of that file. Please, don't forget to also tag the corresponding code chunk as part of your submission!*

```{python}
#| echo: false
#| eval: false

def print_file_contents(file_path):
    """Print contents of a file."""
    try:
        with open(file_path, 'r') as f:
            content = f.read()
            print("```python")
            print(content)
            print("```")
    except FileNotFoundError:
        print("```python")
        print(f"Error: File '{file_path}' not found")
        print("```")
    except Exception as e:
        print("```python") 
        print(f"Error reading file: {e}")
        print("```")

print_file_contents("./top_alerts_map_byhour/app.py") # Change accordingly
```

```{python} 
#| echo: false

# Import required packages.
import pandas as pd
import altair as alt 
import pandas as pd
from datetime import date
import numpy as np
alt.data_transformers.disable_max_rows() 

import json
import re
import requests
from dash import Dash, dcc, html, Input, Output
from shiny import App, render, ui, reactive
```

# Background {-}

## Data Download and Exploration (20 points){-} 

1. 
```{python}
# read in the sample data
df_sample = pd.read_csv("waze_data/waze_data_sample.csv")
```

```{python}
print(df_sample.columns)
```

Variable Names/Data Types:

Unnamed:0     Ordinal 
city          Nominal
confidence    Ordinal
nThumbsUp     Ordinal
street        Nominal
uuid          Ordinal
country       Nominal
type          Nominal
subtype       Ordinal 
roadType      Ordinal
reliability   Ordinal
magvar        Ordinal   
reportRating  Ordinal

2. 
```{python}
# read in the data
df_full = pd.read_csv("waze_data/waze_data.csv")
```

```{python}
# data frame for plotting with null counts for each variable
data = pd.DataFrame({
    'variable': df_full.columns,
    'null_count': df_full.isnull().sum(),
    'non_null_count': df_full.notnull().sum()
})

# melt the data
data_melted = data.melt(
    id_vars='variable', var_name='status', value_name='count')

# Ccreate the bar chart
alt.Chart(data_melted).mark_bar().encode(
    x=alt.X('variable:N', title='Variable'),
    y=alt.Y('count:Q', title='Number of Observations'),
    color=alt.Color('status:N', scale=alt.Scale(domain=['null_count', 'non_null_count'], range=['pink', 'purple']),
                    title='Status', legend=alt.Legend(title="Status"))
).properties(
    title='Null vs. Non-Null Observations per Variable'
)

```

nThumbsUp, street, and subtype have missing values. nThumbsUp has almost all values missing, while subtype has a much smaller number of missing values and street has a few missing values.

3. 
```{python}
#find unique types
unique_types = df_full['type'].unique()
print("Unique values in 'type':", unique_types)
```

```{python}
# find unique subtypes
unique_subtypes = df_full['subtype'].unique()
print("Unique values in 'subtype':", unique_subtypes)
```

```{python}
# how many types have a subtype that is NA? 
subtypes_count_nas = df_full[df_full['subtype'].isna()]['type'].nunique()
print(f"Number of types with NA subtypes: {subtypes_count_nas}") 
```

All four categories in the type columns have subtypes that have NAs.

Traffic
  * Light
  * Moderate
  * Heavy
  * Stand-still
  * Unclassified

Accident
  * Major
  * Minor
  * Unclassified

Road Closed
  * Event
  * Construction
  * Hazard
  * Unclassified

Hazard
  * On Road
      * Stopped car
      * Construction
      * Emergency vehicle
      * Ice
      * Object
      * Pot hole
      * Faulty traffic light
      * Lane closed
      * Road kill
  * On Shoulder
      * Stopped car
      * Animals
      * Missing Sign
  * Weather
      * Flood
      * Fog
      * Heavy snow
      * Hail
  * Unclassified

NAs: 12% of the subtype columns - 96,086 rows - have missing values. This seems like a lot of rows to omit from the data, so I think they should be kept and categorized as 'unclassified'.

```{python}
# replace NAs with "unclassified"
df_full['subtype'] = df_full['subtype'].fillna('Unclassified')
```

4. 
a. 
```{python}
# create the df
crosswalk = df_full.drop_duplicates(subset=["type", "subtype"]).copy()

# keep type and subtype columns - we will add the updated type, subtybe, and subsubtype in next step
crosswalk = crosswalk.loc[:, ["type", "subtype"]]
```

b.
```{python}
crosswalk_data = []

# Loop over the combinations of type and subtype to populate the updated columns
for _, row in crosswalk.iterrows():
    type_val = row['type']
    subtype_val = row['subtype']

    updated_type = None
    updated_subtype = None
    updated_subsubtype = None

    if type_val == 'JAM':
        updated_type = 'Traffic'
        if subtype_val == 'JAM_LIGHT_TRAFFIC':
            updated_subtype = 'Light'
        elif subtype_val == 'JAM_MODERATE_TRAFFIC':
            updated_subtype = 'Moderate'
        elif subtype_val == 'JAM_HEAVY_TRAFFIC':
            updated_subtype = 'Heavy'
        elif subtype_val == 'JAM_STAND_STILL_TRAFFIC':
            updated_subtype = 'Stand-still'
        else:
            updated_subtype = 'Unclassified'
            updated_subsubtype = None

    elif type_val == 'ACCIDENT':
        updated_type = 'Accident'
        if subtype_val == 'ACCIDENT_MAJOR':
            updated_subtype = 'Major'
        elif subtype_val == 'ACCIDENT_MINOR':
            updated_subtype = 'Minor'
        else:
            updated_subtype = 'Unclassified'
            updated_subsubtype = None

    elif type_val == 'ROAD_CLOSED':
        updated_type = 'Road Closed'
        if subtype_val == 'ROAD_CLOSED_EVENT':
            updated_subtype = 'Event'
        elif subtype_val == 'ROAD_CLOSED_CONSTRUCTION':
            updated_subtype = 'Construction'
        elif subtype_val == 'ROAD_CLOSED_HAZARD':
            updated_subtype = 'Hazard'
        else:
            updated_subtype = 'Unclassified'
            updated_subsubtype = None

    elif type_val == 'HAZARD':
        updated_type = 'Hazard'

        if 'HAZARD_ON_ROAD' in subtype_val:
            updated_subtype = 'On Road'
            if 'CAR_STOPPED' in subtype_val:
                updated_subsubtype = 'Stopped car'
            elif 'CONSTRUCTION' in subtype_val:
                updated_subsubtype = 'Construction'
            elif 'EMERGENCY_VEHICLE' in subtype_val:
                updated_subsubtype = 'Emergency vehicle'
            elif 'ICE' in subtype_val:
                updated_subsubtype = 'Ice'
            elif 'OBJECT' in subtype_val:
                updated_subsubtype = 'Object'
            elif 'POT_HOLE' in subtype_val:
                updated_subsubtype = 'Pot hole'
            elif 'TRAFFIC_LIGHT_FAULT' in subtype_val:
                updated_subsubtype = 'Faulty traffic light'
            elif 'LANE_CLOSED' in subtype_val:
                updated_subsubtype = 'Lane closed'
            elif 'ROAD_KILL' in subtype_val:
                updated_subsubtype = 'Road kill'
            else:
                updated_subsubtype = 'Unclassified'

        elif 'HAZARD_ON_SHOULDER' in subtype_val:
            updated_subtype = 'On Shoulder'
            if 'CAR_STOPPED' in subtype_val:
                updated_subsubtype = 'Stopped car'
            elif 'ANIMALS' in subtype_val:
                updated_subsubtype = 'Animals'
            elif 'MISSING_SIGN' in subtype_val:
                updated_subsubtype = 'Missing Sign'
            else:
                updated_subsubtype = 'Unclassified'

        elif 'HAZARD_WEATHER' in subtype_val:
            updated_subtype = 'Weather'
            if 'FLOOD' in subtype_val:
                updated_subsubtype = 'Flood'
            elif 'FOG' in subtype_val:
                updated_subsubtype = 'Fog'
            elif 'HEAVY_SNOW' in subtype_val:
                updated_subsubtype = 'Heavy snow'
            elif 'HAIL' in subtype_val:
                updated_subsubtype = 'Hail'
            else:
                updated_subsubtype = 'Unclassified'

        else:
            updated_subtype = 'Unclassified'
            updated_subsubtype = None

    crosswalk_data.append(
        [type_val, subtype_val, updated_type, updated_subtype, updated_subsubtype])

# Create final crosswalk df
crosswalk = pd.DataFrame(crosswalk_data, columns=[
                         'type', 'subtype', 'updated_type', 'updated_subtype', 'updated_subsubtype'])
```

c.
```{python}
# merge with original data on "type" and "subtype"
merged_data = df_full.merge(crosswalk[['type', 'subtype', 'updated_type', 'updated_subtype', 'updated_subsubtype']],
                            on=['type', 'subtype'], how='left')
```

```{python}
accident_unclassified_rows = merged_data[(merged_data['updated_type'] == 'Accident') &
                                         (merged_data['updated_subtype'] == 'Unclassified')]

num_rows_accident_unclassified = accident_unclassified_rows.shape[0]

print(
    f"Number of rows for 'Accident - Unclassified': {num_rows_accident_unclassified}")
```

```{python}
merged_data.query(
    "updated_type == 'Accident' and updated_subtype == 'Unclassified'").shape[0]
```

There are 24359 rows for Accident - Unclassified.

d. Extra Credit
```{python}
# check that the crosswalk and new merged data have the same values in type and subtype
type_merged = merged_data['type'].unique()
type_crosswalk = crosswalk['type'].unique()

subtype_merged = merged_data['subtype'].unique()
subtype_crosswalk = crosswalk['subtype'].unique()

print("\nUnique 'type' values in merged dataset:", type_merged)
print("Unique 'type' values in crosswalk:", type_crosswalk)

print("\nUnique 'subtype' values in merged dataset:", subtype_merged)
print("Unique 'subtype' values in crosswalk:", subtype_crosswalk)
```

# App #1: Top Location by Alert Type Dashboard (30 points){-}

1. 

a. 
```{python}
# function to extract latitude and longitude
def extract_coordinates(geo):
    match = re.match(r"POINT\((-?\d+\.\d+)\s(-?\d+\.\d+)\)", geo)
    if match:
        return float(match.group(1)), float(match.group(2))
    return None, None


# apply the function to create a new column 'latlon_list'
merged_data['latlon_list'] = merged_data['geo'].apply(extract_coordinates)

# create new columns for latitude and longitude from the list column
merged_data[['latitude', 'longitude']] = pd.DataFrame(
    merged_data['latlon_list'].tolist(), index=merged_data.index)
```

Source: I put the following prompt into ChatGPT --  
"I have a geoWKT column that contains data in this format: POINT(-87.624816 41.753358). Put together a regular expression that extracts the coordinates." and it gave me this: POINT\((-?\d+\.\d+)\s(-?\d+\.\d+)\)

I also used this post: https://stackoverflow.com/questions/35491274/split-a-pandas-column-of-lists-into-multiple-columns

b. 
```{python}
# create the bins
step = 0.01
merged_data["latBin"] = (
    (merged_data.latitude / step).round()).astype(float) * step
merged_data["lonBin"] = (
    (merged_data.longitude / step).round()).astype(float) * step

# make sure all bins rounded to two decimal places
merged_data["latBin"] = merged_data["latBin"].map(lambda x: f"{x:.2f}")
merged_data["lonBin"] = merged_data["lonBin"].map(lambda x: f"{x:.2f}")
```

```{python}
merged_data.to_csv('waze_data/merged_data.csv', index=False)
```

```{python}
# create the groups and see which combination has the greatest number of observations
groups = merged_data.groupby(["latBin", "lonBin"]).size()
max_group = groups.idxmax()
max_count = groups.max()

print(f"The latitude-longitude combination with the greatest number of observations is {max_group} with {max_count} observations.")
```

Source: https://stackoverflow.com/questions/39254704/pandas-group-bins-of-data-per-longitude-latitude

c. 
```{python}
# collapse data to top ten bins for minor accidents and create df
top_alerts_df = (merged_data[(merged_data['type'] == 'ACCIDENT') & (merged_data['subtype'] == 'ACCIDENT_MINOR')]
                 .groupby(['latBin', 'lonBin'])
                 .size()
                 .reset_index(name="alert_count")
                 .sort_values(by="alert_count", ascending=False)
                 .head(10))

# save to csv
top_alerts_df.to_csv('top_alerts_map/top_alerts_map.csv', index=False)
```

The level of aggregation in this case is the number of minor accidents per certain lat/lon bins. There are 10 rows in the dataframe because we filtered to the top 10 lat/lon bins in terms of number of alerts for this chosen type and subtype. 

2. 
```{python}
# create the top ten df for Jam - Heavy Traffic alerts
jam_heavy_traffic_df = (merged_data[(merged_data['type'] == 'JAM') & (merged_data['subtype'] == 'JAM_HEAVY_TRAFFIC')]
                 .groupby(['latBin', 'lonBin'])
                 .size()
                 .reset_index(name="alert_count")
                 .sort_values(by="alert_count", ascending=False)
                 .head(10))

# make the plot
alt.Chart(jam_heavy_traffic_df).mark_circle().encode(
    x=alt.X('lonBin:Q', scale=alt.Scale(domain=[41.87, 41.99]), title='Longitude'),
    y=alt.Y('latBin:Q', scale=alt.Scale(domain=[-87.79, -87.64]), title='Latitude'),
    size=alt.Size(
        'alert_count:Q',
        scale=alt.Scale(domain=[2000, 4500], range=[50, 500]),
        title='Number of Alerts')
).properties(
    title='Top 10 Latitude-Longitude Bins for "Jam - Heavy Traffic" Alerts')
```

3. 
    
a. 
```{python}
url = "https://data.cityofchicago.org/api/geospatial/bbvz-uum9?method=export&format=GeoJSON"
output_file = "/Users/saravanvalkenburgh/Documents/GitHub/pset6/top_alerts_map/chicago_neighborhood_boundaries.geojson"
response = requests.get(url)

if response.status_code == 200:
    with open(output_file, "wb") as file:
        file.write(response.content)
```    

Source: https://realpython.com/python-download-file-from-url/

b. 
```{python}
file_path = "/Users/saravanvalkenburgh/Documents/GitHub/pset6/top_alerts_map/chicago_neighborhood_boundaries.geojson"

with open(file_path) as f:
    chicago_geojson = json.load(f)

geo_data = alt.Data(values=chicago_geojson["features"])
```

4. 

a. 
```{python}

# create the map from the geoJSON file
map_layer = alt.Chart(geo_data).mark_geoshape(
    fill="lightgray",
    stroke="black"
)

# create the scatter plot layer
scatter_layer = alt.Chart(jam_heavy_traffic_df).mark_circle().encode(
    x=alt.X('lonBin:Q', scale=alt.Scale(domain=[41.87, 41.99]), title='Longitude'),
    y=alt.Y('latBin:Q', scale=alt.Scale(domain=[-87.79, -87.64]), title='Latitude'),
    size=alt.Size(
        'alert_count:Q',
        scale=alt.Scale(domain=[2400, 4400], range=[50, 500]),
        title='Number of Alerts'
    ),
    color=alt.value('blue')
)

# Layer the two plots 
layered_plot = (map_layer + scatter_layer).properties(
    title="Chicago Neighborhood Boundaries with 'Jam - Heavy Traffic' Alerts"
)

layered_plot
```

5. 

a. 

```{python}
types_and_subtypes = {
    "Traffic": ["Light", "Moderate", "Heavy", "Stand-still"],
    "Accident": ["Major", "Minor"],
    "Road Closed": ["Event", "Construction", "Hazard"],
    "Hazard": ["On Road", "On Shoulder", "Weather"]
}

app_ui = ui.page_fluid(
    ui.h2("Top 10 Alerts by Type and Subtype"),
    ui.input_select(
        id="type_dropdown",
        label="Select Type:",
        choices=list(types_and_subtypes.keys())
        ui.input_select(
            id="subtype_dropdown",
            label="Select Subtype:",
            choices=[]
        ),
        ui.output_plot("layered_plot")
    ))
```

b. 
```{python}

```

c. 
```{python}

```

d. 
```{python}

```

e. 

# App #2: Top Location by Alert Type and Hour Dashboard (20 points) {-}

1. 

a. 


    
b. 
```{python}

```

c.

```{python}

```
    

2.

a. 



b. 


c. 


# App #3: Top Location by Alert Type and Hour Dashboard (20 points){-}

1. 


a. 

b. 

```{python}

```

2. 

a. 


b. 
    
3. 

a. 
    

b. 


c. 


d.
